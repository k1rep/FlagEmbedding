"""
Input: query
Output: output generated by LLM
Steps:
1. Establish connection with a llm
2. sent msg with the connection
3. get output
"""
import ollama

from abc import ABC, abstractmethod


class BaseGenerator(ABC):
    @abstractmethod
    def generate(self, content):
        pass


class CodeLlamaGenerator(BaseGenerator):
    def __init__(self):
        self.__model = ollama.Client(host='http://localhost:11434')

    def generate(self, content):
        response = self.__model.chat(model='codellama:13b', messages=[
            content
        ])
        return response


if __name__ == "__main__":
    '''
    import ollama
    response = ollama.chat(model='llama3', messages=[
      {
          'role': 'user',
              'content': 'Why is the sky blue?',
                },
                ])
                print(response['message']['content'])
    '''
    response = ollama.chat(model='codellama:13b', messages=[
        {
            'role': 'user',
            'content': 'write me a bubble sort with python',
        },
        ])
    print(response)
    print('='*10)
    print(response['message']['content'])
